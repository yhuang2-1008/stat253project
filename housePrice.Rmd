---
title: "CA House Price Prediction"
author: Jennifer and Eli
output: html_document
---

[Link to kaggle, where we found our data set](https://www.kaggle.com/camnugent/california-housing-prices)

```{r hw2_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message = FALSE, warning = FALSE)
```


```{r, warning = FALSE, message = FALSE, echo = FALSE}
# library statements 
# read in data
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
library(readxl)
tidymodels_prefer()

housing <- read_csv("housing.csv") %>% 
  drop_na()

set.seed(123)
data_cv10<- vfold_cv(housing, v = 10)
```


```{r, include = FALSE, warning = FALSE, message = FALSE, echo = FALSE}
# Re-coding ocean proximity to numeric values 

housing$ocean_proximity[housing$ocean_proximity=="NEAR BAY"] <- 1
housing$ocean_proximity[housing$ocean_proximity=="<1H OCEAN"] <- 2
housing$ocean_proximity[housing$ocean_proximity=="NEAR OCEAN"] <- 3
housing$ocean_proximity[housing$ocean_proximity=="ISLAND"] <- 4
housing$ocean_proximity[housing$ocean_proximity=="INLAND"] <- 5
```


<br><br><br>

## OLS Analysis {-}


```{r, OLS model spec}
lm_spec <-
    linear_reg() %>%
    set_engine(engine = 'lm') %>%
    set_mode('regression')
```



### OLS Recipe and Workflow Setup

```{r, OLS recipe + workflow}
lm_rec <- recipe(median_house_value ~ ., data = housing) %>%
  step_normalize(all_numeric_predictors())

lm_wf <- workflow() %>%
  add_recipe(lm_rec) %>%
  add_model(lm_spec)

housing_lm_wf_1 <- workflow() %>%
  add_recipe(lm_rec) %>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())

housing_lm_wf_2 <- workflow() %>%
  add_formula(median_house_value ~ median_income)%>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())

housing_lm_wf_3 <- workflow() %>%
  add_formula(median_house_value ~ median_income + longitude)%>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())

housing_lm_wf_4 <- workflow() %>%
  add_formula(median_house_value ~ median_income + longitude + latitude)%>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())

housing_lm_wf_5 <- workflow() %>%
  add_formula(median_house_value ~ median_income + longitude + latitude + total_rooms)%>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())

housing_lm_wf_6 <- workflow() %>%
  add_formula(median_house_value ~ median_income + longitude + latitude + total_rooms + population)%>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())
```


### OLS Fit and Tune Model

```{r, OLS: Fit and Tune Model}
mod1 <- fit(lm_spec,
            median_house_value ~ .,
            data = housing)
mod2 <- fit(lm_spec,
            median_house_value ~ median_income,
            data = housing)
mod3 <- fit(lm_spec,
            median_house_value ~ median_income + longitude,
            data = housing)
mod4 <- fit(lm_spec,
            median_house_value ~ median_income + longitude + latitude,
            data = housing)
mod5 <- fit(lm_spec,
            median_house_value ~ median_income + longitude + latitude + total_rooms,
            data = housing)
mod6 <- fit(lm_spec,
            median_house_value ~ median_income + longitude + latitude + total_rooms + population,
            data = housing)

mod1 %>% 
  tidy() %>% 
  slice(-1) %>% 
  mutate(lower = estimate - 1.96*std.error, upper = estimate + 1.96*std.error) %>%
  ggplot() + 
    geom_vline(xintercept=0, linetype=4) + 
    geom_point(aes(x=estimate, y=term)) + 
    geom_segment(aes(y=term, yend=term, x=lower, xend=upper), 
                 arrow = arrow(angle=90, ends='both', length = unit(0.1, 'cm'))) + 
    labs(x = 'Coefficient estimate (95% CI)', y = 'Feature') +
    theme_classic()

mod1 %>%
  tidy()
```


```{r, OLS}
mod1_output <- mod1 %>% 
    predict(new_data = housing) %>%
    bind_cols(housing) %>%
    mutate(resid = median_house_value - .pred)

mod2_output <- mod2 %>% 
    predict(new_data = housing) %>%
    bind_cols(housing) %>%
    mutate(resid = median_house_value - .pred)

mod3_output <- mod3 %>% 
    predict(new_data = housing) %>%
    bind_cols(housing) %>%
    mutate(resid = median_house_value - .pred)

mod4_output <- mod4 %>% 
    predict(new_data = housing) %>%
    bind_cols(housing) %>%
    mutate(resid = median_house_value - .pred)

mod5_output <- mod5 %>% 
    predict(new_data = housing) %>%
    bind_cols(housing) %>%
    mutate(resid = median_house_value - .pred)

mod6_output <- mod6 %>% 
    predict(new_data = housing) %>%
    bind_cols(housing) %>%
    mutate(resid = median_house_value - .pred)
```



```{r, OLS setup mae}
mod1_output %>%
    mae(truth = median_house_value, estimate = .pred)

mod2_output %>%
    mae(truth = median_house_value, estimate = .pred)

mod3_output %>%
    mae(truth = median_house_value, estimate = .pred)

mod4_output %>%
    mae(truth = median_house_value, estimate = .pred)

mod5_output %>%
    mae(truth = median_house_value, estimate = .pred)

mod6_output %>%
    mae(truth = median_house_value, estimate = .pred)
```



### OLS Visualize Model

```{r, OLS visualization with mod1 outputs}
ggplot(mod1_output, aes(y=resid, x=median_house_value)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

#not very useful?
ggplot(mod1_output, aes(y=resid, x=housing_median_age)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

#not very useful 
ggplot(mod1_output, aes(y=resid, x=ocean_proximity)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod1_output, aes(y=resid, x=longitude)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod1_output, aes(y=resid, x=latitude)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod1_output, aes(y=resid, x=total_rooms)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod1_output, aes(y=resid, x=median_income)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod1_output, aes(y=resid, x=total_bedrooms)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()
```


```{r, OLS visualization with mod2 outputs}
ggplot(mod2_output, aes(y=resid, x=total_rooms)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod2_output, aes(y=resid, x=median_house_value)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod2_output, aes(y=resid, x=longitude)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod2_output, aes(y=resid, x=latitude)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod2_output, aes(y=resid, x=median_income)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod2_output, aes(y=resid, x=population)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()
```


```{r, OLS Visualization}
ggplot(mod3_output, aes(y=resid, x=population)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod3_output, aes(y=resid, x=total_rooms)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod6_output, aes(y=resid, x=longitude)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()
```


### OLS Calculate and Collect Metrics

```{r, OLS calculate metrics}
mod1_cv <- fit_resamples(housing_lm_wf_1,
  resamples = data_cv10, 
  metrics = metric_set(rmse, rsq, mae))
  
  
mod2_cv <- fit_resamples(housing_lm_wf_2,
  resamples = data_cv10, 
  metrics = metric_set(rmse, rsq, mae))
  
  
mod3_cv <- fit_resamples(housing_lm_wf_3,
  resamples = data_cv10, 
  metrics = metric_set(rmse, rsq, mae))
  
  
mod4_cv <- fit_resamples(housing_lm_wf_4,
  resamples = data_cv10, 
  metrics = metric_set(rmse, rsq, mae))
  
  
mod5_cv <- fit_resamples(housing_lm_wf_5,
  resamples = data_cv10, 
  metrics = metric_set(rmse, rsq, mae))

  
mod6_cv <- fit_resamples(housing_lm_wf_6,
  resamples = data_cv10, 
  metrics = metric_set(rmse, rsq, mae))
```


```{r, OLS collect metrics}
mod1_cv %>% unnest(.metrics) %>%
  filter(.metric == 'rmse') %>%
  summarize(RMSE_CV = mean(.estimate))

mod2_cv %>% unnest(.metrics) %>%
  filter(.metric == 'rmse') %>%
  summarize(RMSE_CV = mean(.estimate))

mod3_cv %>% unnest(.metrics) %>%
  filter(.metric == 'rmse') %>%
  summarize(RMSE_CV = mean(.estimate))

mod4_cv %>% unnest(.metrics) %>%
  filter(.metric == 'rmse') %>%
  summarize(RMSE_CV = mean(.estimate))

mod5_cv %>% unnest(.metrics) %>%
    filter(.metric == 'rmse') %>%
    summarize(RMSE_CV = mean(.estimate))

mod6_cv %>% unnest(.metrics) %>%
    filter(.metric == 'rmse') %>%
    summarize(RMSE_CV = mean(.estimate))

mod1_cv %>% collect_metrics()
mod2_cv %>% collect_metrics()
mod3_cv %>% collect_metrics()
mod4_cv %>% collect_metrics()
mod5_cv %>% collect_metrics()
mod6_cv %>% collect_metrics()
```



<br><br><br>

## LASSO Analysis {-}

### LASSO Model spec, recipe, and workflow 

```{r, LASSO model spec}

lasso_spec <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso, we'll choose penalty later
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 
```


```{r, lasso recipe + workflow}
lasso_rec <- recipe(median_house_value ~ ., data = housing) %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value
    #step_novel(all_nominal_predictors()) %>% # important if you have rare categorical variables 
    step_normalize(all_numeric_predictors()) %>%  # important standardization step for LASSO
    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables

lasso_wf <- workflow() %>% 
  add_recipe(lasso_rec) %>%
  add_model(lasso_spec)
```


### LASSO Fit and Tune Model


```{r, LASSO: Fit and Tune Model}
# Tune LASSO #1
penalty_grid <- grid_regular(
  penalty(range = c(-5,5)), #log10 transformed 10^-5 to 10^3
  levels = 30)

# Tune LASSO #2
tune_res <- tune_grid( # new function for tuning parameters
  lasso_wf, # workflow
  resamples = data_cv10, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid)

# LASSO model
lasso_fit <- lasso_wf %>% 
  fit(data = housing) # Fit to data
```


### LASSO Calculate and Collect Metrics


```{r, calculate/collect CV metrics}
# plotting LASSO lambda
plot(lasso_fit %>% 
       extract_fit_parsnip() %>% 
       pluck('fit'), # way to get the original glmnet output
     xvar = "lambda")

# Visualize LASSO Metrics from Tuning
autoplot(tune_res) + theme_classic()

# Summarize LASSO CV Metrics
collect_metrics(tune_res) %>%
  filter(.metric == 'mae') %>% # or choose mae
  select(penalty, mae = mean) 

# choose penalty value based on lowest mae or rmse
best_penalty <- select_best(tune_res, metric = 'mae') 
best_penalty
# choose penalty value based on the largest penalty within 1 se of the lowest CV MAE
best_se_penalty <- select_by_one_std_err(tune_res, metric = 'mae', desc(penalty)) 


# Fit Final LASSO Models
final_wf <- finalize_workflow(lasso_wf, best_penalty) # incorporates penalty value to workflow
final_wf_se <- finalize_workflow(lasso_wf, best_se_penalty) # incorporates penalty value to workflow


final_fit <- fit(final_wf, data = housing)
final_fit_se <- fit(final_wf_se, data = housing)

tidy(final_fit)
tidy(final_fit_se)
```

- use estimate to talk abiout how changes in coefficient leads to changes in response variable


### LASSO Visualize residuals 

```{r, LASSO visual residuals}
lasso_mod_out <- final_fit_se %>%
    predict(new_data = housing) %>%
    bind_cols(housing) %>%
    mutate(resid = median_house_value - .pred)

lasso_mod_out %>% 
  ggplot(aes(x = .pred, y = resid)) + 
  geom_point() +
  geom_smooth(se = FALSE) + 
  geom_hline(yintercept = 0) + 
  theme_classic()
```

- there's a straight line cuz of the cap at 500000 for housing value

$~$

C.Compare estimated test performance across the models. Which models(s) might you prefer?

- Using the measures calculated using the ordinary least squares. Also, using the average cross validated rmse model 1 performed the bed. The first model including all the variables is the best. This is also validated using the best lambda measure calculated using lasso. 
 
$~$
 
D. Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships.

- Latitude and longitude might be better modeled with a nonlinear relationship as there appears to be two spikes in their residual plots. 

$~$

E. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. Do the methods you’ve applied reach consensus on which variables are most important? What insights are expected? Surprising?

- The most important predictor according to the lasso is median income. It survives the longest. Median income also has the lowest p value. This is not surprising as a higher income would suggest the neighboring household's ability to purchase a more expensive home. Using p value population the relationship observed in population has the second lowest probability of being caused by random chance. All that's being said, however, p value is not always the best indicator of which variables to include in the model.


<br><br><br>

### Summarize Investigations 

Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?

- The model that contains all of the variables in the data set is the best model so far. Based on the fact that that model yielded the lowest CV test error rate, it seems like taking into account of house age, number of rooms, location, ocean proximity, total bedrooms, population, and income level of that block of houses is important to determine house value. 

- The interpretability of our model is pretty staright forward, though we would like to find out if some individual variables could have more influence on house value and make predictions with that variable. 

$~$
$~$

### Societal Impact

Are there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work?

- The variable `median_income` measures the median income for households within a block of houses. Since this variable seems to be the most persistent in our model, according to our LASSO results, this confirms the seriously large wage gap in the United States. Income affects one's social status, health care access, and even house value. We could further infer from our preliminary analysis that income affects house value because it is more likely that those who have a higher income are not criminals. Where criminal activity is low, housing options there attracts more buyers --- especially buyers of higher social status. 

- The data was derived from Aurélien Géron's book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It contains the 1990 California census information, specifically on housing. We don't suspect that the way in which the data was collected is harmful. But we do need to acknowledge that those who did respond to census letters are probably those who have a mailing address and those who are middle to higher income groups. Back in the days, I don't think there census surveys are delivered eletronically and therefore data collection must have had missed people who do not have a mailing address or P.O. box. Additionally, people who are middle to higher income groups would more likely respond to the census because they can read and write English, they might be more informed about the purpose of a census survey, and they might just have more time to attend to a list of questions printed on multiple pages of paper. People who had lower income and probably had to work so many more hours just to meet ends and the census letter could be the last on their minds. And keep in mind that the data set is specifically on California housing, which is arguably one of the more wealthy and liberal states in the US. 


<br><br><br>

<br><br><br>



## Project Work Part 2 {-}

2. **Accounting for non-linearity**
    - Update your models to use natural splines for some of the quantitative predictors to account for non-linearity (these are GAMs).
        - I recommend using OLS engine to fit these final models.
        - You'll need to update the recipe to include `step_ns()` for each quantitative predictor that you want to allow to be non-linear.
        - To determine number of knots (`deg_free`), I recommend fitting a smoothing spline and use `edf` to inform your choice.


```{r}
# Linear Regression Model Spec
lm_spec <- 
  linear_reg() %>%
  set_engine(engine = 'lm') %>%
  set_mode('regression')

lm_rec <- recipe(median_house_value ~ ., data = housing)

ns_rec <- lm_rec %>%
  step_ns(longitude, deg_free = 10) %>% 
  step_ns(latitude, deg_free = 10) %>% 
  step_ns(housing_median_age, deg_free = 10) %>% 
  step_ns(total_rooms, deg_free = 10) %>% 
  step_ns(total_bedrooms, deg_free = 10) %>% 
  step_ns(households, deg_free = 10) %>% 
  step_ns(population, deg_free = 10)

#degrees of freedom has to be a whole number and larger than edf 
  
ns_wf <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(ns_rec)
data_cv10<- vfold_cv(housing, v = 10)

cv_output <- fit_resamples(
  ns_wf,
  resamples = data_cv10, # cv folds
  metrics = metric_set(mae))

cv_output %>% collect_metrics()

# Fit with all data
# ns_mod <- fit(
#   lm_spec, #workflow
#   data = housing)

```

$~$

```{r, GAM}
gam_spec <- 
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression') 

gam_mod <- fit(gam_spec,
    median_house_value ~ ocean_proximity + s(longitude) + s(latitude) + 
      s(housing_median_age) + s(total_rooms) + s(total_bedrooms) + 
      s(population) + s(households) + s(median_income),
    data = housing)
```

$~$

```{r, diagnostics}
par(mfrow=c(2,2))

gam_mod %>% pluck('fit') %>% mgcv::gam.check() 

gam_mod %>% pluck('fit') %>% summary() 
```
$~$

a. Compare insights from variable importance analyses here and the corresponding results from the Investigation 1. Now after having accounted for non-linearity, have the most relevant predictors changed?

- Having accounted for non-linearity, it looks like every variable in our data set is important. However even though each variable is important, their magnitudes are different. For example, median_income has a greater magnitude (F = 1179.025) than does total_rooms (F = 8.413) even though they both have near-zero p-value. 

$~$

b. Do you gain any insights from the GAM output plots (easily obtained from fitting smoothing splines) for each predictor?

- The GAM output plots show us that total_rooms, total_bedrooms, households, and population become less accurate in predicting near the upper ends. Population seems to be the least accurate in predicting using smoothing splines.

```{r}
gam_mod %>% pluck('fit') %>% plot(page = 1)
```



$~$

c. Compare model performance between your GAM models and the models that assumes linearity.

- Using MAE as our performance metric our GAM model is better than the model that assumes linearity using the same variables.

$~$

d. How does test performance of the GAMs compare to other models you explored?

Below are the MAEs yeilded from our models thus far: 
1) OLS: 49797.67
2) LASSO: 49791.43	
3) GAMs: 44580.23

- Our GAMs model's test performance is better than those of OLS and LASSO. Variables such as longitude, latitude, total_rooms, and population had non-linear trends so GAMs being able to account for that resulted in a better MAE. 


<br>

3. **Summarize investigations**
    - Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?



<br>

4. **Societal impact**

Are there any harms that may come from your analyses and/or how the data were collected?

- Things that we had previously discussed in HW2 still holds true for our investigation in HW3. Median_income is still the most influencial predictor for house price and has the greatest magnitude of effect. This confirms the wage gap in the United States and how that affects one's social status, health care access, and real estae value. 

$~$ 

What cautions do you want to keep in mind when communicating your work?

- We want to be cautious that our analyses from this dataset does not apply to all of the state of California. This data likely only applies to the wealthier regions of the Claifornia. Additionally, our dataset does not have information regarding the race/ethnicity and education level of the homeowner. These two variables would likely reveal disparaties in house value despite being in a wealth California region. And we also need to keep in mind of the nature of census data. People who respond to census are either aware of the use of census data, have a mailing address, or both. 



<br><br><br>




