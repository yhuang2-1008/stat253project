---
title: "Homework 2"
author: Jennnifer Huang and Eli Ivanov
output: html_document
---


```{r hw2_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, message = FALSE, warning = FALSE)
```

### Required Analyses {-}

1. **Initial investigation: ignoring nonlinearity (for now)**
    a. Use ordinary least squares (OLS) by using the `lm` engine and LASSO (`glmnet` engine) to build  a series of initial regression models for your quantitative outcome as a function of the predictors of interest. (As part of data cleaning, exclude any variables that you don't want to consider as predictors.)
        - You'll need two model specifications, `lm_spec` and `lm_lasso_spec` (you'll need to tune this one).
    b. For each set of variables, you'll need a `recipe` with the `formula`, `data`, and pre-processing steps
        - You may want to have steps in your recipe that remove variables with near zero variance (`step_nzv()`), remove variables that are highly correlated with other variables (`step_corr()`), normalize all quantitative predictors (`step_normalize(all_numeric_predictors())`) and add indicator variables for any categorical variables (`step_dummy(all_nominal_predictors())`).
        - These models should not include any transformations to deal with nonlinearity. You'll explore this in the next investigation.
    c. Estimate the test performance of the models using CV. Report and interpret (with units) the CV metric estimates along with a measure of uncertainty in the estimate (`std_error` is readily available when you used `collect_metrics(summarize=TRUE)`).
        - Compare estimated test performance across the models. Which models(s) might you prefer?
    d. Use residual plots to evaluate whether some quantitative predictors might be better modeled with nonlinear relationships.
    e. Which variables do you think are the most important predictors of your quantitative outcome? Justify your answer. Do the methods you've applied reach consensus on which variables are most important? What insights are expected? Surprising?
        - Note that if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected.

<br>

#### Your Work {-}

a & b.

```{r, warning = FALSE, message = FALSE, echo = FALSE}
# library statements 
# read in data
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
library(readxl)

nba <- read_excel("NBA Player Data and All Star Votes Jan10.xlsx") %>% 
  select(-Player, -Tm, -"All-Star Votes")


nba$Pos[nba$Pos=="PG"] <- 1
nba$Pos[nba$Pos=="SG-PG"] <- 2
nba$Pos[nba$Pos=="SF"] <- 3
nba$Pos[nba$Pos=="SG-PF"] <- 4
nba$Pos[nba$Pos=="SF-PF"] <- 5
nba$Pos[nba$Pos=="PF"] <- 6
nba$Pos[nba$Pos=="C"] <- 7

nba$Conference[nba$Conference=="Western"] <- 0
nba$Conference[nba$Conference=="Eastern"] <- 1

nba$`Position Group`[nba$`Position Group`=="Frontcourt"] <- 0
nba$`Position Group`[nba$`Position Group`=="Backcourt"] <- 1



```

```{r}
# creation of cv folds
cv10 <- vfold_cv(nba, v = 10)
```

```{r}
# model spec
lm_spec <-
    linear_reg() %>%
    set_engine(engine = 'lm') %>%
    set_mode('regression')

lm_lasso_spec <-
    linear_reg() %>%
    set_args(mixture = 1, penalty = tune()) %>%
    set_engine(engine = 'glmnet') %>%
    set_mode('regression')

penalty_grid <- grid_regular(
    penalty(range = c(-5, 3)), #log10 transformed 10^-5 to 10^3
    levels = 5)

```

```{r}
# recipes & workflows
ride_recipe <- recipe(MP ~ `TS%` + `TRB%` + `AST%` + `TOV%`, data = nba) %>%
  update_role(all_nominal(), new_role = "ID") %>%
  update_role(MP, new_role = "outcome") %>%
  update_role(`TS%`,`TRB%`,`AST%`,`TOV%`, new_role = "predictor") %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value
    step_novel(all_nominal_predictors()) %>% # important if you have rare categorical variables
    step_normalize(all_numeric_predictors()) %>%  # important standardization step for LASSO
    step_dummy(all_nominal_predictors())

lasso_wf_tune <- workflow() %>%
    add_recipe(ride_recipe) %>%
    add_model(lm_lasso_spec)

tmp <- fit(lasso_wf_tune, data = nba)
```

```{r}
# fit & tune models
tune_res <- tune_grid( # new function for tuning parameters
    lasso_wf_tune, # workflow
    resamples = cv10, # cv folds
    metrics = metric_set(rmse, mae),
    grid = penalty_grid) # penalty grid defined above

collect_metrics(tune_res) %>%
    filter(.metric == 'rmse') %>% # or choose mae
    select(penalty, rmse = mean)

mod1_cv <- fit_resamples(lasso_wf_tune,
                         resamples = cv10,
                         metrics = metric_set(rmse, rsq, mae))

autoplot(tune_res) + theme_classic()

best_penalty <- select_best(tune_res, metric = 'rmse')
```

c.

```{r}
#  calculate/collect CV metrics

```

 
d.

```{r}
# visual residuals

```

e.

<br>

2. **Summarize investigations**
    - Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?



<br>

3. **Societal impact**
    - Are there any harms that may come from your analyses and/or how the data were collected?
    - What cautions do you want to keep in mind when communicating your work?


